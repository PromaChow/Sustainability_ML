{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Files Averages: {'Nr': 264.61538461538464, 'NCSS': 21.573870573870575, 'CCN': 2.6788766788766787}\n",
      "Non-Test Files Averages: {'Nr': 1063.180339985218, 'NCSS': 64.58610495195862, 'CCN': 7.144124168514413}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "# Define the input file path\n",
    "input_file = \"/Users/promachowdhury/Downloads/code-metrics (6)/lizard_report.xml\"\n",
    "\n",
    "# Read and clean the XML file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Find the starting position of the XML declaration\n",
    "xml_start = content.find(\"<?xml version\")\n",
    "if xml_start != -1:\n",
    "    content = content[xml_start:]\n",
    "\n",
    "# Parse the XML content\n",
    "root = ET.fromstring(content)\n",
    "\n",
    "# Separate test and non-test files\n",
    "test_metrics = {\"Nr\": [], \"NCSS\": [], \"CCN\": []}\n",
    "non_test_metrics = {\"Nr\": [], \"NCSS\": [], \"CCN\": []}\n",
    "\n",
    "# Process each <item> in the XML\n",
    "for item in root.findall(\".//item\"):\n",
    "    name = item.get(\"name\")\n",
    "    values = [int(val.text) for val in item.findall(\"value\")[:3]] \n",
    "\n",
    "    if name and values:\n",
    "        if re.search(r\"\\btests?\\b\", name): \n",
    "            test_metrics[\"Nr\"].append(values[0])\n",
    "            test_metrics[\"NCSS\"].append(values[1])\n",
    "            test_metrics[\"CCN\"].append(values[2])\n",
    "        else:  \n",
    "            non_test_metrics[\"Nr\"].append(values[0])\n",
    "            non_test_metrics[\"NCSS\"].append(values[1])\n",
    "            non_test_metrics[\"CCN\"].append(values[2])\n",
    "\n",
    "def compute_average(metrics):\n",
    "    return {key: sum(vals) / len(vals) if vals else 0 for key, vals in metrics.items()}\n",
    "\n",
    "\n",
    "test_averages = compute_average(test_metrics)\n",
    "non_test_averages = compute_average(non_test_metrics)\n",
    "\n",
    "\n",
    "print(\"Test Files Averages:\", test_averages)\n",
    "print(\"Non-Test Files Averages:\", non_test_averages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Files Averages: {'h1': 2.36318407960199, 'h2': 20.970149253731343, 'N1': 17.442786069651742, 'N2': 28.577114427860696, 'vocabulary': 23.333333333333332, 'length': 46.01990049751244, 'calculated_length': 140.25828960295004, 'volume': 289.19051389257027, 'difficulty': 1.6826429523514541, 'effort': 1173.0530507260382, 'time': 65.16961392922434, 'bugs': 0.09639683796419021}\n",
      "Non-Test Files Averages: {'h1': 2.9452054794520546, 'h2': 23.246575342465754, 'N1': 28.1324200913242, 'N2': 42.74429223744292, 'vocabulary': 26.19178082191781, 'length': 70.87671232876713, 'calculated_length': 167.73122775125947, 'volume': 395.22573348395633, 'difficulty': 3.7062815018465995, 'effort': 3649.069801036498, 'time': 202.7261000575831, 'bugs': 0.13174191116131886}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File path\n",
    "file_path = \"/Users/promachowdhury/Downloads/code-metrics (6)/halstead.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "test_values = {}\n",
    "non_test_values = {}\n",
    "\n",
    "\n",
    "for file_path, metrics in data.items():\n",
    "    is_test = \"/tests/\" in file_path \n",
    "    category = test_values if is_test else non_test_values\n",
    "\n",
    "    for metric, value in metrics[\"total\"].items():\n",
    "        if metric not in category:\n",
    "            category[metric] = []\n",
    "        category[metric].append(value)\n",
    "\n",
    "\n",
    "def compute_avg(values_dict):\n",
    "    return {metric: sum(values) / len(values) if values else 0 for metric, values in values_dict.items()}\n",
    "\n",
    "\n",
    "test_averages = compute_avg(test_values)\n",
    "non_test_averages = compute_avg(non_test_values)\n",
    "\n",
    "\n",
    "print(\"Test Files Averages:\", test_averages)\n",
    "print(\"Non-Test Files Averages:\", non_test_averages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Files Averages: {'loc': 77.30845771144278, 'lloc': 34.35820895522388, 'sloc': 48.35820895522388, 'comments': 17.62686567164179, 'multi': 0.0, 'blank': 11.472636815920398, 'single_comments': 17.47761194029851}\n",
      "Non-Test Files Averages: {'loc': 200.324200913242, 'lloc': 86.70319634703196, 'sloc': 147.6027397260274, 'comments': 17.904109589041095, 'multi': 0.0, 'blank': 35.67579908675799, 'single_comments': 17.04566210045662}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File path\n",
    "file_path = \"/Users/promachowdhury/Downloads/code-metrics (6)/raw_metrics.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize dictionaries to store metric values separately for test and non-test files\n",
    "test_values = {}\n",
    "non_test_values = {}\n",
    "\n",
    "# Process each file in the JSON\n",
    "for file_path, metrics in data.items():\n",
    "    is_test = \"/tests/\" in file_path  # Identify test files\n",
    "    category = test_values if is_test else non_test_values\n",
    "\n",
    "    for metric, value in metrics.items():\n",
    "        if metric not in category:\n",
    "            category[metric] = []\n",
    "        category[metric].append(value)\n",
    "\n",
    "# Function to compute averages\n",
    "def compute_avg(values_dict):\n",
    "    return {metric: sum(values) / len(values) if values else 0 for metric, values in values_dict.items()}\n",
    "\n",
    "# Compute separate averages\n",
    "test_averages = compute_avg(test_values)\n",
    "non_test_averages = compute_avg(non_test_values)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Files Averages:\", test_averages)\n",
    "print(\"Non-Test Files Averages:\", non_test_averages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Code Complexity Analysis Results (Test Files):\n",
      "----------------------------------------------\n",
      "Average Method Complexity: 4.23\n",
      "Average Class Complexity: 6.67\n",
      "Total Methods: 624\n",
      "Total Classes: 6\n",
      "Highest Method Complexity: 221\n",
      "Lowest Method Complexity: 1\n",
      "\n",
      "Code Complexity Analysis Results (Non-Test Files):\n",
      "--------------------------------------------------\n",
      "Average Method Complexity: 4.34\n",
      "Average Class Complexity: 5.20\n",
      "Total Methods: 1098\n",
      "Total Classes: 203\n",
      "Highest Method Complexity: 78\n",
      "Lowest Method Complexity: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "def calculate_complexity_metrics(data: Dict[str, List[Dict]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate average complexity metrics separately for test and non-test files.\n",
    "\n",
    "    Args:\n",
    "        data: Dictionary with filenames as keys and lists of code elements as values.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing separate complexity metrics for test and non-test files.\n",
    "    \"\"\"\n",
    "    test_methods, test_classes = [], []\n",
    "    non_test_methods, non_test_classes = [], []\n",
    "\n",
    "    # Collect complexity values separately for test and non-test files\n",
    "    for filename, elements in data.items():\n",
    "        is_test_file = \"/tests/\" in filename  # Identify test files\n",
    "        \n",
    "        for element in elements:\n",
    "            if element[\"type\"] in [\"function\", \"method\"]:\n",
    "                if is_test_file:\n",
    "                    test_methods.append(element[\"complexity\"])\n",
    "                else:\n",
    "                    non_test_methods.append(element[\"complexity\"])\n",
    "            elif element[\"type\"] == \"class\":\n",
    "                if is_test_file:\n",
    "                    test_classes.append(element[\"complexity\"])\n",
    "                else:\n",
    "                    non_test_classes.append(element[\"complexity\"])\n",
    "\n",
    "    # Function to compute complexity stats\n",
    "    def compute_metrics(methods: List[int], classes: List[int]) -> Dict[str, Union[float, int]]:\n",
    "        return {\n",
    "            \"average_method_complexity\": sum(methods) / len(methods) if methods else 0,\n",
    "            \"average_class_complexity\": sum(classes) / len(classes) if classes else 0,\n",
    "            \"total_methods\": len(methods),\n",
    "            \"total_classes\": len(classes),\n",
    "            \"max_method_complexity\": max(methods) if methods else 0,\n",
    "            \"min_method_complexity\": min(methods) if methods else 0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"test_files\": compute_metrics(test_methods, test_classes),\n",
    "        \"non_test_files\": compute_metrics(non_test_methods, non_test_classes),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load the JSON file\n",
    "    with open('/Users/promachowdhury/Downloads/code-metrics (6)/complexity.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    try:\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_complexity_metrics(data)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nCode Complexity Analysis Results (Test Files):\")\n",
    "        print(f\"----------------------------------------------\")\n",
    "        print(f\"Average Method Complexity: {metrics['test_files']['average_method_complexity']:.2f}\")\n",
    "        print(f\"Average Class Complexity: {metrics['test_files']['average_class_complexity']:.2f}\")\n",
    "        print(f\"Total Methods: {metrics['test_files']['total_methods']}\")\n",
    "        print(f\"Total Classes: {metrics['test_files']['total_classes']}\")\n",
    "        print(f\"Highest Method Complexity: {metrics['test_files']['max_method_complexity']}\")\n",
    "        print(f\"Lowest Method Complexity: {metrics['test_files']['min_method_complexity']}\")\n",
    "\n",
    "        print(f\"\\nCode Complexity Analysis Results (Non-Test Files):\")\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        print(f\"Average Method Complexity: {metrics['non_test_files']['average_method_complexity']:.2f}\")\n",
    "        print(f\"Average Class Complexity: {metrics['non_test_files']['average_class_complexity']:.2f}\")\n",
    "        print(f\"Total Methods: {metrics['non_test_files']['total_methods']}\")\n",
    "        print(f\"Total Classes: {metrics['non_test_files']['total_classes']}\")\n",
    "        print(f\"Highest Method Complexity: {metrics['non_test_files']['max_method_complexity']}\")\n",
    "        print(f\"Lowest Method Complexity: {metrics['non_test_files']['min_method_complexity']}\")\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
